{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "TeujBmn0XXg5"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Let's build a cross encoder - Step by step guide\n",
    "\n",
    "We'll use it for the STS task. We'll use the pretrained BERT model for transfer learning on this new semantic sim task."
   ],
   "metadata": {
    "id": "f2o4qpipu2HH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8a44BH7iuxSE",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:27.868602Z",
     "start_time": "2024-03-30T15:40:26.069973Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging, os, sys, json, torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# we'll define or model name here\n",
    "transformer_model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "id": "J_qZxcuXvCEB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# before writting any code we're going to need our tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name, strip_accents=False)"
   ],
   "metadata": {
    "id": "1o_vFobN3IFv",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:28.496541Z",
     "start_time": "2024-03-30T15:40:27.868602Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file, test=False):\n",
    "        self.tokenizer = tokenizer  # we'll need this in the __getitem__ function\n",
    "        self.instances = []\n",
    "        df = pd.read_csv(file)\n",
    "        for index, row in df.iterrows():\n",
    "            if test:\n",
    "                self.instances.append({\n",
    "                    \"title\": row['title'],\n",
    "                    \"content\": row['content']\n",
    "                })\n",
    "            else:\n",
    "                self.instances.append({\n",
    "                    \"title\": row['title'],\n",
    "                    \"content\": row['content'],\n",
    "                    \"class\": row['class']\n",
    "                })\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)  # return how many instances we have. It's a list after all\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.instances[index]"
   ],
   "metadata": {
    "id": "g8mEoGAhu_7-",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:28.500474Z",
     "start_time": "2024-03-30T15:40:28.496541Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test it's working. Load a dataset and print the first example."
   ],
   "metadata": {
    "id": "Up2FrrOrCHAg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create the MyDataset object with the test_data\n",
    "test_dataset = MyDataset(tokenizer, \"train.csv\")\n",
    "instance = test_dataset[0]  # this calls our __getitem__(0) method\n",
    "\n",
    "# now let's print what it contains:\n",
    "for key in instance:\n",
    "  print(f\"{key}: {instance[key]}\")"
   ],
   "metadata": {
    "id": "v9XHkW97CEJl",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:30.733053Z",
     "start_time": "2024-03-30T15:40:28.500474Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: PSD în alertă\n",
      "content: Prăbușirea PSD de la altitudinea sigură a celor 70% în incertitudinea legată de păstrarea guvernării a stîrnit groază în partid. Vulnerabil, instabil și pus în fața propriei prostii, PSD duce o luptă vizibilă cu depresia. Baronii fornăie ca o turmă cuprinsă de neliniște, pe care un singur zgomot brusc o poate pune pe fugă. Socotelile sînt clare: dacă pierde președinția, USD pierde tot, iar coșmarul revenirii lui Băsescu la putere pe căi democratice devine tot mai palpabil. Consultanții Sultănoiu, Teodorescu, Palada și Dîncu caută febrili o candidatură pentru Cotroceni, fiindcă Ponta n-ar vrea să lase guvernul. Pe de altă parte, sondajele estimează o victorie a Pisicului azi, dar nu și în noiembrie. Oprescu și Isărescu sînt plimbați prin fața eșantioanelor naționale, dar nici unul nu prezintă garanții destule partidului. Partidul dorește să-și vadă liderul pe tron, iar liderul înțelege că, dacă nu-și asumă responsabilitatea candidaturii, autoritatea i se fîsîie. În sfîrșit, imbecilii care au stăruit ca PSD să acapareze cît mai multe ministere prin ruperea USL văd grozăvia în care se găsește viitorul lor politic. În cazul în care va fi forțat să candideze, Ponta ia în calcul o ieșire din funcția de premier prin august, ca să se dedice campaniei și ca să se îndepărteze de ura firească pe care o trezește Guvernul. Postul ar rămîne în posesia unui vicepremier, probabil Dragnea, care să atragă fulgerele nemulțumirii asupra lui. Pe scurt, spaima în care se zbate PSD nu e egalată decît de veselia inconștientă a PNL.\n",
      "class: True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to collate the instances in a batch."
   ],
   "metadata": {
    "id": "ixO5x6JUDl-t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MyCollator(object):\n",
    "    def __init__(self, tokenizer, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len  # this will be our model's maximum sequence length\n",
    "        self.tokenizer = tokenizer   # we still need our tokenizer to know that the pad token's id is\n",
    "\n",
    "    def __call__(self, input_batch):\n",
    "        titles = []\n",
    "        contents = []\n",
    "        labels = []  # Initialize labels as an empty list\n",
    "\n",
    "        # Check if 'class' is in instance before appending\n",
    "        for instance in input_batch:\n",
    "            titles.append(instance['title'])\n",
    "            contents.append(instance['content'])\n",
    "            if 'class' in instance:  # Only add label if 'class' exists\n",
    "                labels.append(instance['class'])\n",
    "\n",
    "        tokenized_batch = self.tokenizer(\n",
    "            list(map(lambda x: f\"[CLS]{x[0]}[SEP]{x[1]}[SEP]\", zip(titles, contents))),\n",
    "            padding=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Convert labels to a tensor only if labels are not empty\n",
    "        if labels:\n",
    "            labels = torch.tensor(labels, dtype=torch.long)  # Changed dtype to torch.long for classification\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return {\n",
    "            \"tokenized_batch\": tokenized_batch,\n",
    "            \"labels\": labels\n",
    "        }"
   ],
   "metadata": {
    "id": "OJ_LbCpzCEtv",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:30.737058Z",
     "start_time": "2024-03-30T15:40:30.733053Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# let's test our collator\n",
    "test_dataset = MyDataset(tokenizer, \"train.csv\")\n",
    "my_collator = MyCollator(tokenizer=tokenizer, max_seq_len=512)\n",
    "\n",
    "# crete a dataloader and get first batch of 3\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=3, collate_fn=my_collator)\n",
    "\n",
    "iterable_data = iter(test_dataloader)\n",
    "first_batch = next(iterable_data) # this is the output_batch from above\n",
    "for key in first_batch:\n",
    "  print(f\"{key} is a {first_batch[key]}\")"
   ],
   "metadata": {
    "id": "U2vJvl8Hv02G",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:33.019317Z",
     "start_time": "2024-03-30T15:40:30.737563Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_batch is a {'input_ids': tensor([[   2,    2, 2761,  ...,    0,    0,    0],\n",
      "        [   2,    2,  538,  ...,   18,    3,    3],\n",
      "        [   2,    2, 4140,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "labels is a tensor([1, 1, 1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model preparation\n",
    "\n",
    "We're finally here :)\n",
    "\n",
    "As we're using Pytorch Lightning to do the behind-the-scenes training, we do need to define a few functions: \n",
    "\n",
    "* ``__init__``, ``forward``\n",
    "* ``training_step``\n",
    "* ``validation_step``\n",
    "* ``configure_optimizers``\n",
    "\n",
    "As this is a single block of code, comments will be inline:\n"
   ],
   "metadata": {
    "id": "wUHxArRSvEuN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_classes, lr=2e-05, model_max_length=512):\n",
    "        super().__init__()\n",
    "        # Load model, tokenizer, and configure the new output layer for classification\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.output_layer = nn.Linear(768, num_classes)  # Assuming 768 is the hidden size\n",
    "        self.loss_fct = CrossEntropyLoss()  # Changed to CrossEntropyLoss for classification\n",
    "        self.lr = lr\n",
    "        self.model_max_length = model_max_length\n",
    "      \n",
    "    def forward(self, tokenized_batch):\n",
    "        # we're just wrapping the code on the AutoModelForTokenClassification\n",
    "        # it needs the input_ids, attention_mask and labels\n",
    "\n",
    "        output = self.model(\n",
    "            input_ids=tokenized_batch['input_ids'],\n",
    "            attention_mask=tokenized_batch['attention_mask'],\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooler_output = output['pooler_output']  # [batch_size, 768]\n",
    "        prediction = self.output_layer(pooler_output)  # [batch_size, 1]\n",
    "\n",
    "        return prediction.flatten()\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tokenized_batch = batch['tokenized_batch']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        prediction = self.forward(tokenized_batch)  # [batch_size, 1]\n",
    "        \n",
    "        loss = self.loss_fct(prediction, labels)\n",
    "\n",
    "        self.log(\"train_loss\", loss.detach().cpu().item(), on_step=True, on_epoch=True, prog_bar=True,)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tokenized_batch = batch['tokenized_batch']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        prediction = self.forward(tokenized_batch)  # [batch_size, seq_len, 768]\n",
    "       \n",
    "        loss = self.loss_fct(prediction, labels)\n",
    "\n",
    "        self.log(\"val_loss\", loss.detach().cpu().item(), on_step=True, on_epoch=True, prog_bar=True,)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW([p for p in self.parameters() if p.requires_grad], lr=self.lr, eps=1e-08)"
   ],
   "metadata": {
    "id": "fuiCMO5kv65w",
    "ExecuteTime": {
     "end_time": "2024-03-30T15:40:33.023739Z",
     "start_time": "2024-03-30T15:40:33.019317Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training phase\n",
    "\n",
    "At this point we're ready to start training. When the code is ready, switch your colab to GPU, and run every cell up to this point, to have the training run on the GPU. Notice that Pytorch Lightning abstracts all the hassle of training on different devices. \n",
    "\n",
    "So, what do we need?\n",
    "\n",
    "We need the model itself (the ``TransformerModel`` object), and the trainer object which receives a few parameters detailed below. The trainer will move the data on GPU automatically, call ``train_step`` and ``train_epoch_end``, then do the same for validation, and then do backprop (internally calls Pytorch's ``.backward()``, ``optimizer_step`` and ``zero_grad`` to update the model weights. It also handles all the gritty stuff like early stopping, logging, model saving, distributed training (if you have more than 1 GPU), etc.\n"
   ],
   "metadata": {
    "id": "GC_AqbTkvGvI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = TransformerModel(\n",
    "    model_name=transformer_model_name,\n",
    "    lr=2e-5,\n",
    "    model_max_length=512,\n",
    "    num_classes=2  # we have 2 classes\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # devices=-1,  # uncomment this when training on gpus\n",
    "    accelerator=\"gpu\",  # uncomment this when training on gpus\n",
    "    max_epochs=2,  # set this to -1 when training fully \n",
    "    #limit_train_batches=10,  # comment this out when training fully\n",
    "    #limit_val_batches=5,  # comment this out when training fully\n",
    "    gradient_clip_val=1.0,\n",
    "    enable_checkpointing=False  # this disables saving the model each epoch\n",
    ")\n",
    "\n",
    "# instantiate dataloaders\n",
    "# a batch_size of 8 should work fine on 16GB GPUs\n",
    "train_dataloader = DataLoader(MyDataset(tokenizer, \"train.csv\"), batch_size=64, collate_fn=my_collator, shuffle=True, pin_memory=True, num_workers=20)\n",
    "validation_dataloader = DataLoader(MyDataset(tokenizer, \"test.csv\", test=True), batch_size=64, collate_fn=my_collator, shuffle=False, pin_memory=True, num_workers=20)\n",
    "\n",
    "# call this to start training\n",
    "trainer.fit(model, train_dataloader, validation_dataloader)"
   ],
   "metadata": {
    "id": "Lr7Fl8CHv5_j",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-30T15:40:33.023739Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Alex\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | BertModel        | 124 M \n",
      "1 | output_layer | Linear           | 1.5 K \n",
      "2 | loss_fct     | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.772   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44ef404d72094d2ea8b2750724c3c92a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's use our model"
   ],
   "metadata": {
    "id": "jr5iQ_ybvIst"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solution (hidden)"
   ],
   "metadata": {
    "id": "TeujBmn0XXg5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict (model, sent1, sent2):\n",
    "    concatenated_sentences = f\"[CLS]{sent1.strip()}[SEP]{sent2.strip()}[SEP]\"\n",
    "\n",
    "    tokenized_batch = model.tokenizer([concatenated_sentences], padding=True, max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    predictions = model.forward(tokenized_batch)  # returns a [batch_size, ]\n",
    "    \n",
    "    return predictions[0].item()*5.  # select the first item and multiply by 5"
   ],
   "metadata": {
    "id": "G8cINri0Xbj5",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "id": "z9RmJ3ucX2Qb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# let's test our code\n",
    "model.eval()\n",
    "\n"
   ],
   "metadata": {
    "id": "EUApiUX2Nseg",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tX47C7TwX7it",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
