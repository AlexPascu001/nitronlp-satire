{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:40:34.186035Z",
     "start_time": "2024-03-31T11:40:33.932119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "'NVIDIA GeForce RTX 3090'"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from transformers import Trainer, TrainingArguments, AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "# get name of the device\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model = 'dumitrescustefan/bert-base-romanian-cased-v1'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T17:30:46.320351Z",
     "start_time": "2024-03-30T17:30:46.317144Z"
    }
   },
   "id": "287f819c562b6172",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:40:41.580149Z",
     "start_time": "2024-03-31T11:40:36.666122Z"
    }
   },
   "id": "2a3ab7164af4d06c",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          id                                              title  \\\n0          0                                      PSD în alertă   \n1          1  În amintirea Vioricăi, milioane de români beau...   \n2          2  Dramă! Când credea că nu se poate mai rău, un ...   \n3          3  Spania - România, 5-0. „Tricolorii”, îngenunch...   \n4          4              Campanie electorală, veselie generală   \n...      ...                                                ...   \n70570  70570  Spații de cazare la mare înălțime. Refugiul Ie...   \n70571  70571  Sfântul Vali, colanții de la mall și miorița n...   \n70572  70572  Banii sănătății unde e, banii sănătății ce-ați...   \n70573  70573  Pentru că nu poate da o lege să nu mai bată vâ...   \n70574  70574               Ascuțiți Facebook-ul, vin alegerile!   \n\n                                                 content  class  \n0      Prăbușirea PSD de la altitudinea sigură a celo...   True  \n1      Moțiunea de cenzură care a doborât guvernul Dă...   True  \n2      Credeai că ai ajuns la fundul sacului? Înseamn...   True  \n3      Echipa națională a României a fost umilită, lu...  False  \n4      Toate cresc în campania electorală, cît n-au c...   True  \n...                                                  ...    ...  \n70570  Pentru drumeția de azi, un echipaj de interven...   True  \n70571  Pe Mădălin nu l-a lăsat tac-su să termine opt ...   True  \n70572  Reforma sănătății a început, în România, pe vr...   True  \n70573  Premierul Mihai Tudose a fost, din nou, deranj...   True  \n70574  Visul oricărui politician, fie el local sau aj...   True  \n\n[70575 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>content</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>PSD în alertă</td>\n      <td>Prăbușirea PSD de la altitudinea sigură a celo...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>În amintirea Vioricăi, milioane de români beau...</td>\n      <td>Moțiunea de cenzură care a doborât guvernul Dă...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Dramă! Când credea că nu se poate mai rău, un ...</td>\n      <td>Credeai că ai ajuns la fundul sacului? Înseamn...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Spania - România, 5-0. „Tricolorii”, îngenunch...</td>\n      <td>Echipa națională a României a fost umilită, lu...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Campanie electorală, veselie generală</td>\n      <td>Toate cresc în campania electorală, cît n-au c...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70570</th>\n      <td>70570</td>\n      <td>Spații de cazare la mare înălțime. Refugiul Ie...</td>\n      <td>Pentru drumeția de azi, un echipaj de interven...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70571</th>\n      <td>70571</td>\n      <td>Sfântul Vali, colanții de la mall și miorița n...</td>\n      <td>Pe Mădălin nu l-a lăsat tac-su să termine opt ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70572</th>\n      <td>70572</td>\n      <td>Banii sănătății unde e, banii sănătății ce-ați...</td>\n      <td>Reforma sănătății a început, în România, pe vr...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70573</th>\n      <td>70573</td>\n      <td>Pentru că nu poate da o lege să nu mai bată vâ...</td>\n      <td>Premierul Mihai Tudose a fost, din nou, deranj...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70574</th>\n      <td>70574</td>\n      <td>Ascuțiți Facebook-ul, vin alegerile!</td>\n      <td>Visul oricărui politician, fie el local sau aj...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>70575 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:40:41.587039Z",
     "start_time": "2024-03-31T11:40:41.580149Z"
    }
   },
   "id": "778b16a64da0a043",
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "13.973332955479355"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the average number of words in a title\n",
    "df['title'].apply(lambda x: len(x.split())).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:41:36.252221Z",
     "start_time": "2024-03-31T11:41:36.191694Z"
    }
   },
   "id": "dcf5081466000e43",
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "332.3609261201009"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'].dropna().apply(lambda x: len(x.split())).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:43:12.306861Z",
     "start_time": "2024-03-31T11:43:11.314512Z"
    }
   },
   "id": "39979853bfbd4bc0",
   "execution_count": 137
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_12496\\1972564188.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['content'] = df['content'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# make the 'content' column type string\n",
    "df['content'] = df['content'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:42:47.049085Z",
     "start_time": "2024-03-31T11:42:47.037053Z"
    }
   },
   "id": "3214cf55eee3918f",
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[134], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# count the average number of words in a content, if the content is empty, the number of words is 0\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m!=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\series.py:4915\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4780\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4781\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4782\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4787\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4788\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4790\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4791\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4906\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4907\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4908\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4909\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4910\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4911\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4912\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4913\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4914\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4915\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[134], line 2\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# count the average number of words in a content, if the content is empty, the number of words is 0\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlen\u001B[39m(\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m()) \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mmean()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# count the average number of words in a content, if the content is empty, the number of words is 0\n",
    "df['content'].apply(lambda x: len(x.split()) if x != '' else 0).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:42:19.235091Z",
     "start_time": "2024-03-31T11:42:19.183745Z"
    }
   },
   "id": "7f3d738271bcead8",
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# remove rows with empty title\n",
    "df = df.dropna(subset=['title'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T11:41:34.336548Z",
     "start_time": "2024-03-31T11:41:34.315401Z"
    }
   },
   "id": "35dc14da0350cfc",
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          id                                              title  \\\n0          0                                      PSD în alertă   \n1          1  În amintirea Vioricăi, milioane de români beau...   \n2          2  Dramă! Când credea că nu se poate mai rău, un ...   \n3          3  Spania - România, 5-0. „Tricolorii”, îngenunch...   \n4          4              Campanie electorală, veselie generală   \n...      ...                                                ...   \n70570  70570  Spații de cazare la mare înălțime. Refugiul Ie...   \n70571  70571  Sfântul Vali, colanții de la mall și miorița n...   \n70572  70572  Banii sănătății unde e, banii sănătății ce-ați...   \n70573  70573  Pentru că nu poate da o lege să nu mai bată vâ...   \n70574  70574               Ascuțiți Facebook-ul, vin alegerile!   \n\n                                                 content  class  \n0      Prăbușirea PSD de la altitudinea sigură a celo...   True  \n1      Moțiunea de cenzură care a doborât guvernul Dă...   True  \n2      Credeai că ai ajuns la fundul sacului? Înseamn...   True  \n3      Echipa națională a României a fost umilită, lu...  False  \n4      Toate cresc în campania electorală, cît n-au c...   True  \n...                                                  ...    ...  \n70570  Pentru drumeția de azi, un echipaj de interven...   True  \n70571  Pe Mădălin nu l-a lăsat tac-su să termine opt ...   True  \n70572  Reforma sănătății a început, în România, pe vr...   True  \n70573  Premierul Mihai Tudose a fost, din nou, deranj...   True  \n70574  Visul oricărui politician, fie el local sau aj...   True  \n\n[70574 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>content</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>PSD în alertă</td>\n      <td>Prăbușirea PSD de la altitudinea sigură a celo...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>În amintirea Vioricăi, milioane de români beau...</td>\n      <td>Moțiunea de cenzură care a doborât guvernul Dă...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Dramă! Când credea că nu se poate mai rău, un ...</td>\n      <td>Credeai că ai ajuns la fundul sacului? Înseamn...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Spania - România, 5-0. „Tricolorii”, îngenunch...</td>\n      <td>Echipa națională a României a fost umilită, lu...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Campanie electorală, veselie generală</td>\n      <td>Toate cresc în campania electorală, cît n-au c...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70570</th>\n      <td>70570</td>\n      <td>Spații de cazare la mare înălțime. Refugiul Ie...</td>\n      <td>Pentru drumeția de azi, un echipaj de interven...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70571</th>\n      <td>70571</td>\n      <td>Sfântul Vali, colanții de la mall și miorița n...</td>\n      <td>Pe Mădălin nu l-a lăsat tac-su să termine opt ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70572</th>\n      <td>70572</td>\n      <td>Banii sănătății unde e, banii sănătății ce-ați...</td>\n      <td>Reforma sănătății a început, în România, pe vr...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70573</th>\n      <td>70573</td>\n      <td>Pentru că nu poate da o lege să nu mai bată vâ...</td>\n      <td>Premierul Mihai Tudose a fost, din nou, deranj...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>70574</th>\n      <td>70574</td>\n      <td>Ascuțiți Facebook-ul, vin alegerile!</td>\n      <td>Visul oricărui politician, fie el local sau aj...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>70574 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T16:35:23.812667Z",
     "start_time": "2024-03-30T16:35:23.808658Z"
    }
   },
   "id": "8813f16f56f3f495",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T16:58:42.321081Z",
     "start_time": "2024-03-30T16:58:40.433568Z"
    }
   },
   "id": "65582fc047793800",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# preprocess the data for the model\n",
    "def replace_unwanted_characters(df):\n",
    "    df['content'] = df['content'].fillna('')\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\"))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\"))\n",
    "    return df\n",
    "\n",
    "repeated_characters = [x * 3 for x in \"AĂÂBCDEFGHÎJKLMNOPQRSȘTȚUVWXYZaăâbcdefghîjklmnopqrsștțuvwxyz,.-/';[]-!@#$%^&*()?+\"]\n",
    "repeated_characters.append('iiii')\n",
    "repeated_characters.append('IIII')\n",
    "\n",
    "profanity_list = [\n",
    "    \"muie\",\n",
    "    \"laba\",\n",
    "    \"labă\",\n",
    "    \"pula\",\n",
    "    \"pulă\",\n",
    "    \"pizda\",\n",
    "    \"pizdă\",\n",
    "    \"ce pula\",\n",
    "    \"ce pulă\",\n",
    "    \"ce pizda\",\n",
    "    \"ce pizdă\",\n",
    "    \"caca\",\n",
    "    \"cacat\",\n",
    "    \"căcat\",\n",
    "    \"pipi\",\n",
    "    \"pisat\",\n",
    "    \"pișat\",\n",
    "    \"pishat\",\n",
    "    \"rahat\",\n",
    "    \"kkt\",\n",
    "    \"kk\",\n",
    "    \"plm\",\n",
    "    \"ma fut\",\n",
    "    \"mă fut\",\n",
    "    \"ma cac\",\n",
    "    \"mă cac\",\n",
    "    \"ma pis\",\n",
    "    \"mă pis\",\n",
    "    \"ma pish\",\n",
    "    \"mă pish\",\n",
    "    \"pwla\",\n",
    "    \"pwlă\",\n",
    "    \"p.u.l.a.\",\n",
    "    \"poola\",\n",
    "    \"naiba\",\n",
    "    \"dracu\",\n",
    "    \"draq\",\n",
    "    \"drecu\",\n",
    "    \"naibii\",\n",
    "    \"dracului\",\n",
    "    \"drecului\",\n",
    "    \"drqlui\",\n",
    "    \"coaie\",\n",
    "    \"coae\",\n",
    "    \"sloboz\",\n",
    "    \"lindic\",\n",
    "    \"gaoz\",\n",
    "    \"ochiul maro\",\n",
    "    \"floci\",\n",
    "    \"cur\",\n",
    "    \"futai\",\n",
    "    \"futare\",\n",
    "    \"futere\",\n",
    "    \"popou\",\n",
    "    \"nanau\",\n",
    "    \"pulii\",\n",
    "    \"pulii mele\",\n",
    "    \"coaiele\",\n",
    "    \"coaiele mele\",\n",
    "    \"pulile\",\n",
    "    \"măta\",\n",
    "    \"mata\",\n",
    "    \"mă-tii\",\n",
    "    \"mă-ta\",\n",
    "    \"mă-ti\",\n",
    "]\n",
    "\n",
    "english_words = set(words.words())\n",
    "\n",
    "def normalize_text(df):\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(r\"<.*?>\", '', x ))\n",
    "    df['content'] = df['content'].apply(lambda x:re.sub(r\"<.*?>\", '', x))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(r\"\\.\\.\\.\", '', x ))\n",
    "    df['content'] = df['content'].apply(lambda x:re.sub(r\"\\.\\.\\.\", '', x))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(r\"#[\\w+-]+\", '[HTAG]', x ))\n",
    "    df['content'] = df['content'].apply(lambda x:re.sub(r\"#[\\w+-]+\", '[HTAG]', x))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : ' '.join([\"[STAR]\" if \"*\" in word else word for word in x.split()]))\n",
    "    df['content'] = df['content'].apply(lambda x:' '.join([\"[STAR]\" if \"*\" in word else word for word in x.split()]))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : ' '.join([\"[REP]\" if any([rep in word for rep in repeated_characters]) else word for word in x.split()]))\n",
    "    df['content'] = df['content'].apply(lambda x : ' '.join([\"[REP]\" if any([rep in word for rep in repeated_characters]) else word for word in x.split()]))\n",
    "\n",
    "    pattern = r'^[!?\"\\',“”-]*(.*?)[!?\"\\',“”-]*$'\n",
    "    df['title'] = df['title'].apply(lambda x : ' '.join([\"[PROF]\" if re.sub(pattern, r'\\1', word) in profanity_list else word for word in x.split()]))\n",
    "    df['content'] = df['content'].apply(lambda x : ' '.join([\"[PROF]\" if re.sub(pattern, r'\\1', word) in profanity_list else word for word in x.split()]))\n",
    "\n",
    "\n",
    "    emoji_pattern = r\"\"\"\n",
    "                    (?:\n",
    "                      [<>]?\n",
    "                      [:;=8]                     # eyes\n",
    "                      [\\-o\\*\\']?                 # optional nose\n",
    "                      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "                      |\n",
    "                      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "                      [\\-o\\*\\']?                 # optional nose\n",
    "                      [:;=8]                     # eyes\n",
    "                      [<>]?\n",
    "                      |\n",
    "                      </?3                       # heart\n",
    "                    )\"\"\"\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(emoji_pattern, '[EMOJI]', x ))\n",
    "    df['content'] = df['content'].apply(lambda x : re.sub(emoji_pattern, '[EMOJI]', x ))\n",
    "\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T23:13:21.260548Z",
     "start_time": "2024-03-30T23:13:21.143667Z"
    }
   },
   "id": "a79ecdc0b728d68b",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_titles_tokens(df):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    aux = list(zip(df['title'].tolist(), df['class'].tolist()))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[STAR]', '[HTAG]', '[REP]', '[PROF]', '[EMOJI]']})\n",
    "\n",
    "    robert = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    robert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    for title, label in tqdm(aux):\n",
    "        input_ids = torch.tensor(tokenizer.encode(title, add_special_tokens=True, max_length=32, padding='max_length', truncation=True, return_tensors=\"np\"))\n",
    "        outputs = robert(input_ids)\n",
    "        mean_pooling = torch.mean(outputs[0], dim = 1).squeeze(0).detach()\n",
    "        X_train.append(mean_pooling)\n",
    "        y_train.append(label)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def get_content_tokens(df):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    aux = list(zip(df['content'].tolist(), df['class'].tolist()))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[STAR]', '[HTAG]', '[REP]', '[PROF]', '[EMOJI]']})\n",
    "\n",
    "    robert = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    robert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    for content, label in tqdm(aux):\n",
    "        if len(content) > 0:\n",
    "            input_ids = torch.tensor(tokenizer.encode(content, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors=\"np\"))\n",
    "            outputs = robert(input_ids)\n",
    "            mean_pooling = torch.mean(outputs[0], dim = 1).squeeze(0).detach()\n",
    "            X_train.append(mean_pooling)\n",
    "            y_train.append(label)\n",
    "        else:\n",
    "            X_train.append([0 for _ in range(768)])\n",
    "            y_train.append(label)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def preprocess_dataset(df, name):\n",
    "    df = replace_unwanted_characters(df)\n",
    "    df = normalize_text(df)\n",
    "\n",
    "    text_embeddings, _ = get_titles_tokens(df)\n",
    "    content_embeddings, _ = get_content_tokens(df)\n",
    "\n",
    "    np.save(name+'title.npy', text_embeddings)\n",
    "    np.save(name+'content.npy', content_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T23:13:21.952038Z",
     "start_time": "2024-03-30T23:13:21.937492Z"
    }
   },
   "id": "d0d94764bce3d574",
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17643 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 17643/17643 [06:12<00:00, 47.34it/s]\n",
      "100%|██████████| 17643/17643 [52:10<00:00,  5.64it/s] \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('second.csv')\n",
    "preprocess_dataset(df, 'second')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T00:12:16.320669Z",
     "start_time": "2024-03-30T23:13:23.151757Z"
    }
   },
   "id": "18754ca31c3aed04",
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[115], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m df_test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mpreprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[113], line 56\u001B[0m, in \u001B[0;36mpreprocess_dataset\u001B[1;34m(df, name)\u001B[0m\n\u001B[0;32m     53\u001B[0m df \u001B[38;5;241m=\u001B[39m replace_unwanted_characters(df)\n\u001B[0;32m     54\u001B[0m df \u001B[38;5;241m=\u001B[39m normalize_text(df)\n\u001B[1;32m---> 56\u001B[0m text_embeddings, _ \u001B[38;5;241m=\u001B[39m \u001B[43mget_titles_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m content_embeddings, _ \u001B[38;5;241m=\u001B[39m get_content_tokens(df)\n\u001B[0;32m     59\u001B[0m np\u001B[38;5;241m.\u001B[39msave(name\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle.npy\u001B[39m\u001B[38;5;124m'\u001B[39m, text_embeddings)\n",
      "Cell \u001B[1;32mIn[113], line 4\u001B[0m, in \u001B[0;36mget_titles_tokens\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m      2\u001B[0m X_train \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m y_train \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 4\u001B[0m aux \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist(), \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtolist()))\n\u001B[0;32m      6\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdumitrescustefan/bert-base-romanian-cased-v1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m added_toks \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39madd_special_tokens({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madditional_special_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[STAR]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[HTAG]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[REP]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[PROF]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[EMOJI]\u001B[39m\u001B[38;5;124m'\u001B[39m]})\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'class'"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "preprocess_dataset(df_test, 'test')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T00:12:51.735141Z",
     "start_time": "2024-03-31T00:12:16.321671Z"
    }
   },
   "id": "4270c549daa549ec",
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17643/17643 [05:54<00:00, 49.71it/s]\n",
      "100%|██████████| 17643/17643 [46:01<00:00,  6.39it/s]\n"
     ]
    }
   ],
   "source": [
    "df_third = pd.read_csv('third.csv')\n",
    "preprocess_dataset(df_third, 'third')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T01:16:07.745320Z",
     "start_time": "2024-03-31T00:23:44.358604Z"
    }
   },
   "id": "a186ad50efff2240",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_train_in_six(dataset):\n",
    "    length = int(len(dataset) / 6)\n",
    "\n",
    "    first = dataset[:length]\n",
    "    second = dataset[length:2*length]\n",
    "    third = dataset[2*length:3*length]\n",
    "    fourth = dataset[3*length:4*length]\n",
    "    fifth = dataset[4*length:5*length]\n",
    "    sixth = dataset[5*length:]\n",
    "\n",
    "    first.to_csv('first_test.csv', encoding='utf-8', index=False)\n",
    "    second.to_csv('second_test.csv', encoding='utf-8', index=False)\n",
    "    third.to_csv('third_test.csv', encoding='utf-8', index=False)\n",
    "    fourth.to_csv('fourth_test.csv', encoding='utf-8', index=False)\n",
    "    fifth.to_csv('fifth_test.csv', encoding='utf-8', index=False)\n",
    "    sixth.to_csv('sixth_test.csv', encoding='utf-8', index=False)\n",
    "    \n",
    "\n",
    "split_train_in_six(pd.read_csv('test.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T02:26:26.757451Z",
     "start_time": "2024-03-31T02:26:24.763059Z"
    }
   },
   "id": "2b3dcec579a18e66",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_titles_tokens(df):\n",
    "    X_train = []\n",
    "    aux = df['title'].tolist()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[STAR]', '[HTAG]', '[REP]', '[PROF]', '[EMOJI]']})\n",
    "\n",
    "    robert = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    robert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    for title in tqdm(aux):\n",
    "        input_ids = torch.tensor(tokenizer.encode(title, add_special_tokens=True, max_length=32, padding='max_length', truncation=True, return_tensors=\"np\"))\n",
    "        outputs = robert(input_ids)\n",
    "        mean_pooling = torch.mean(outputs[0], dim = 1).squeeze(0).detach()\n",
    "        X_train.append(mean_pooling)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "\n",
    "    return X_train\n",
    "\n",
    "def get_content_tokens(df):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    aux = df['content'].tolist()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[STAR]', '[HTAG]', '[REP]', '[PROF]', '[EMOJI]']})\n",
    "\n",
    "    robert = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "    robert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    for content in tqdm(aux):\n",
    "        if len(content) > 0:\n",
    "            input_ids = torch.tensor(tokenizer.encode(content, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors=\"np\"))\n",
    "            outputs = robert(input_ids)\n",
    "            mean_pooling = torch.mean(outputs[0], dim = 1).squeeze(0).detach()\n",
    "            X_train.append(mean_pooling)\n",
    "        else:\n",
    "            X_train.append([0 for _ in range(768)])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def preprocess_dataset(df, name):\n",
    "    df = replace_unwanted_characters(df)\n",
    "    df = normalize_text(df)\n",
    "\n",
    "    text_embeddings = get_titles_tokens(df)\n",
    "    content_embeddings = get_content_tokens(df)\n",
    "\n",
    "    np.save(name+'title.npy', text_embeddings)\n",
    "    np.save(name+'content.npy', content_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T02:28:02.194453Z",
     "start_time": "2024-03-31T02:28:02.189880Z"
    }
   },
   "id": "461564207f907539",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6111/6111 [02:11<00:00, 46.47it/s]\n",
      "100%|██████████| 6111/6111 [12:26<00:00,  8.19it/s]\n"
     ]
    }
   ],
   "source": [
    "df_fourth_test = pd.read_csv('fourth_test.csv')\n",
    "preprocess_dataset(df_fourth_test, 'fourth_test')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T02:54:29.882684Z",
     "start_time": "2024-03-31T02:39:43.938673Z"
    }
   },
   "id": "35f1edc57a3c8485",
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6111/6111 [02:10<00:00, 46.78it/s]\n",
      "100%|██████████| 6111/6111 [12:02<00:00,  8.45it/s]\n"
     ]
    }
   ],
   "source": [
    "df_fifth_test = pd.read_csv('fifth_test.csv')\n",
    "preprocess_dataset(df_fifth_test, 'fifth_test')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T03:08:51.529663Z",
     "start_time": "2024-03-31T02:54:29.882684Z"
    }
   },
   "id": "112b34f821a885e2",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a347176b10ccb0f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
