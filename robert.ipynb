{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:44:25.213457Z",
     "start_time": "2024-03-30T20:44:22.510683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import nltk\n",
    "from transformers import Trainer\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:44:25.216181Z",
     "start_time": "2024-03-30T20:44:25.213967Z"
    }
   },
   "id": "c9f14b310050f1a6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "# romanian_words = set()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:44:25.265017Z",
     "start_time": "2024-03-30T20:44:25.216181Z"
    }
   },
   "id": "b35ecafb20acba58"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_normalized.csv')\n",
    "df_test = pd.read_csv('test_normalized.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:44:32.309736Z",
     "start_time": "2024-03-30T20:44:30.930304Z"
    }
   },
   "id": "bd517c5f208b2bf4",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# preprocess the data for the model\n",
    "def replace_unwanted_characters(df):\n",
    "    df.loc[df['content'].isnull(), 'content'] = ''\n",
    "    df.loc[df['title'].isnull(), 'title'] = ''\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\"))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\"))\n",
    "    return df\n",
    "\n",
    "repeated_characters = [x * 3 for x in \"AĂÂBCDEFGHÎJKLMNOPQRSȘTȚUVWXYZaăâbcdefghîjklmnopqrsștțuvwxyz,.-/';[]-!@#$%^&*()?+\"]\n",
    "repeated_characters.append('iiii')\n",
    "\n",
    "profanity_list = [\n",
    "    \"muie\",\n",
    "    \"laba\",\n",
    "    \"labă\",\n",
    "    \"pula\",\n",
    "    \"pulă\",\n",
    "    \"pizda\",\n",
    "    \"pizdă\",\n",
    "    \"ce pula\",\n",
    "    \"ce pulă\",\n",
    "    \"ce pizda\",\n",
    "    \"ce pizdă\",\n",
    "    \"caca\",\n",
    "    \"cacat\",\n",
    "    \"căcat\",\n",
    "    \"pipi\",\n",
    "    \"pisat\",\n",
    "    \"pișat\",\n",
    "    \"pishat\",\n",
    "    \"rahat\",\n",
    "    \"kkt\",\n",
    "    \"kk\",\n",
    "    \"plm\",\n",
    "    \"ma fut\",\n",
    "    \"mă fut\",\n",
    "    \"ma cac\",\n",
    "    \"mă cac\",\n",
    "    \"ma pis\",\n",
    "    \"mă pis\",\n",
    "    \"ma pish\",\n",
    "    \"mă pish\",\n",
    "    \"pwla\",\n",
    "    \"pwlă\",\n",
    "    \"p.u.l.a.\",\n",
    "    \"poola\",\n",
    "    \"naiba\",\n",
    "    \"dracu\",\n",
    "    \"draq\",\n",
    "    \"drecu\",\n",
    "    \"naibii\",\n",
    "    \"dracului\",\n",
    "    \"drecului\",\n",
    "    \"drqlui\",\n",
    "    \"coaie\",\n",
    "    \"coae\",\n",
    "    \"sloboz\",\n",
    "    \"lindic\",\n",
    "    \"gaoz\",\n",
    "    \"ochiul maro\",\n",
    "    \"floci\",\n",
    "    \"cur\",\n",
    "    \"futai\",\n",
    "    \"futare\",\n",
    "    \"futere\",\n",
    "    \"popou\",\n",
    "    \"nanau\",\n",
    "    \"pulii\",\n",
    "    \"pulii mele\",\n",
    "    \"coaiele\",\n",
    "    \"coaiele mele\",\n",
    "    \"pulile\",\n",
    "    \"măta\",\n",
    "    \"mata\",\n",
    "    \"mă-tii\",\n",
    "    \"mă-ta\",\n",
    "    \"mă-ti\",\n",
    "]\n",
    "\n",
    "english_words = set(words.words())\n",
    "\n",
    "def normalize_text(df):\n",
    "\n",
    "    # replace special characters\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('ă', 'a'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('â', 'a'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('î', 'i'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('ș', 's'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('ț', 't'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('ă', 'a'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('â', 'a'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('î', 'i'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('ș', 's'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('ț', 't'))\n",
    "    # again with capital letters\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('Ă', 'A'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('Â', 'A'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('Î', 'I'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('Ș', 'S'))\n",
    "    df['content'] = df['content'].apply(lambda x: x.replace('Ț', 'T'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('Ă', 'A'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('Â', 'A'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('Î', 'I'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('Ș', 'S'))\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace('Ț', 'T'))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(r\"<.*?>\", '', x ))\n",
    "    df['content'] = df['content'].apply(lambda x:re.sub(r\"<.*?>\", '', x))\n",
    "\n",
    "    # df['title'] = df['title'].apply(lambda x : re.sub(r\"\\.\\.\\.\", '', x ))\n",
    "    # df['content'] = df['content'].apply(lambda x:re.sub(r\"\\.\\.\\.\", '', x))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(r\"#[\\w+-]+\", '[HTAG]', x ))\n",
    "    df['content'] = df['content'].apply(lambda x:re.sub(r\"#[\\w+-]+\", '[HTAG]', x))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x : ' '.join([\"[STAR]\" if \"*\" in word else word for word in x.split()]))\n",
    "    df['content'] = df['content'].apply(lambda x:' '.join([\"[STAR]\" if \"*\" in word else word for word in x.split()]))\n",
    "\n",
    "    # df['title'] = df['title'].apply(lambda x : ' '.join([\"[REP]\" if any([rep in word for rep in repeated_characters]) else word for word in x.split()]))\n",
    "    # df['content'] = df['content'].apply(lambda x : ' '.join([\"[REP]\" if any([rep in word for rep in repeated_characters]) else word for word in x.split()]))\n",
    "\n",
    "    pattern = r'^[!?\"\\',“”-]*(.*?)[!?\"\\',“”-]*$'\n",
    "    df['title'] = df['title'].apply(lambda x : ' '.join([\"[PROF]\" if re.sub(pattern, r'\\1', word) in profanity_list else word for word in x.split()]))\n",
    "    df['content'] = df['content'].apply(lambda x : ' '.join([\"[PROF]\" if re.sub(pattern, r'\\1', word) in profanity_list else word for word in x.split()]))\n",
    "\n",
    "\n",
    "    emoji_pattern = r\"\"\"\n",
    "                    (?:\n",
    "                      [<>]?\n",
    "                      [:;=8]                     # eyes\n",
    "                      [\\-o\\*\\']?                 # optional nose\n",
    "                      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "                      |\n",
    "                      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "                      [\\-o\\*\\']?                 # optional nose\n",
    "                      [:;=8]                     # eyes\n",
    "                      [<>]?\n",
    "                      |\n",
    "                      </?3                       # heart\n",
    "                    )\"\"\"\n",
    "    df['title'] = df['title'].apply(lambda x : re.sub(emoji_pattern, '[EMOJI]', x ))\n",
    "    df['content'] = df['content'].apply(lambda x : re.sub(emoji_pattern, '[EMOJI]', x ))\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def elim_cuv_scurte(df):\n",
    "    df['content'] = df['content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>=2]))\n",
    "    df['title'] = df['title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>=2]))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "train_df = replace_unwanted_characters(df)\n",
    "train_df = normalize_text(train_df)\n",
    "train_df = elim_cuv_scurte(train_df)\n",
    "\n",
    "test_df = replace_unwanted_characters(df_test)\n",
    "test_df = normalize_text(test_df)\n",
    "test_df = elim_cuv_scurte(test_df)\n",
    "\n",
    "train_df.loc[train_df['content'].isnull(), 'content'] = ''\n",
    "train_df.loc[train_df['title'].isnull(), 'title'] = ''\n",
    "test_df.loc[test_df['content'].isnull(), 'content'] = ''\n",
    "test_df.loc[test_df['title'].isnull(), 'title'] = ''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:05:18.224519Z",
     "start_time": "2024-03-31T04:03:44.623378Z"
    }
   },
   "id": "a329ace241999a0f"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "train_df.to_csv('train_normalized.csv', index=False)\n",
    "test_df.to_csv('test_normalized.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:26:27.706689Z",
     "start_time": "2024-03-30T20:26:25.714424Z"
    }
   },
   "id": "291df0092dbb1fef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_normalized.csv')\n",
    "df_test = pd.read_csv('test_normalized.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:51:32.665995Z",
     "start_time": "2024-03-30T20:51:31.201116Z"
    }
   },
   "id": "babcd91eb44fa680",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "df_shuffled = train_df.sample(frac=0.3, random_state=1000)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(df_shuffled))\n",
    "df_train = df_shuffled[:train_size]\n",
    "df_val = df_shuffled[train_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:06:57.752867Z",
     "start_time": "2024-03-31T04:06:57.744095Z"
    }
   },
   "id": "9141c9ce9d904bc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_train.loc[train_df['content'].isnull(), 'content'] = ''\n",
    "df_train.loc[train_df['title'].isnull(), 'title'] = ''\n",
    "df_val.loc[train_df['content'].isnull(), 'content'] = ''\n",
    "df_val.loc[train_df['title'].isnull(), 'title'] = ''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:06:59.499548Z",
     "start_time": "2024-03-31T04:06:59.484490Z"
    }
   },
   "id": "c32acd397ac947f2",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# fill the missing values\n",
    "df_train.loc[df_train['content'].isnull(), 'content'] = ''\n",
    "df_train.loc[df_train['title'].isnull(), 'title'] = ''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:07:00.038536Z",
     "start_time": "2024-03-31T04:07:00.034515Z"
    }
   },
   "id": "82054c4c310d1bd4",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_val.loc[train_df['content'].isnull(), 'content'] = ''\n",
    "df_val.loc[train_df['title'].isnull(), 'title'] = ''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:08:25.385694Z",
     "start_time": "2024-03-31T04:08:25.370742Z"
    }
   },
   "id": "7594e59f8e4a268f",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Series([], Name: content, dtype: object)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find where the missing values are\n",
    "df_val.loc[df_val['content'].isnull(), 'content']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:09:14.839015Z",
     "start_time": "2024-03-31T04:09:14.835009Z"
    }
   },
   "id": "fda02c0b84cf70df",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[STAR]', '[HTAG]', '[REP]', '[PROF]', '[EMOJI]']})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:09:37.450592Z",
     "start_time": "2024-03-31T04:09:36.836996Z"
    }
   },
   "id": "bc8d92ac0d0f9558"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataframe.reset_index()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        title = self.dataframe.loc[index]['title']\n",
    "        content = self.dataframe.loc[index]['content']\n",
    "        label = self.dataframe.loc[index]['class']\n",
    "\n",
    "        title = '[CLS] ' + title + ' [SEP] ' + content\n",
    "\n",
    "        title_tensor = self.tokenizer(title, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
    "        return title_tensor, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:09:37.453368Z",
     "start_time": "2024-03-31T04:09:37.450592Z"
    }
   },
   "id": "e85f24d1bf4fad8d"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "class CustomRoBert(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(CustomRoBert, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "        self.transformer.resize_token_embeddings(len(tokenizer))\n",
    "        # self.pooling = None # add pooling method\n",
    "        self.classifier = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        outputs  = self.transformer(input_ids = batch['input_ids'].squeeze(1), attention_mask=batch['attention_mask'].squeeze(1))\n",
    "        # outputs = self.pooling(outputs)\n",
    "        outputs = self.classifier(outputs[1])\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:09:37.905016Z",
     "start_time": "2024-03-31T04:09:37.902508Z"
    }
   },
   "id": "86555742a24bee27"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "training_dataset = CustomDataset(df_train, tokenizer)\n",
    "validation_dataset = CustomDataset(df_val, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, sampler=RandomSampler(training_dataset),batch_size=32)\n",
    "val_dataloader = DataLoader(validation_dataset, sampler=SequentialSampler(validation_dataset), batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:09:38.733880Z",
     "start_time": "2024-03-31T04:09:38.729302Z"
    }
   },
   "id": "13f86ae3d9436e3b"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T20:54:16.880457Z",
     "start_time": "2024-03-30T20:54:16.877949Z"
    }
   },
   "id": "9bfcaeaa24de0df1"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 103,681 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = CustomRoBert().to(device)\n",
    "\n",
    "for p in model.transformer.parameters():\n",
    "    p.requires_grad = False\n",
    "trainable_params_transformer = [p for (n, p) in model.transformer.named_parameters() if \"bias\" in n]\n",
    "for p in trainable_params_transformer:\n",
    "    p.requires_grad = True\n",
    "trainable_params = list(model.classifier.parameters())\n",
    "trainable_params.extend(list(trainable_params_transformer))\n",
    "\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(trainable_params, lr= 1e-5)\n",
    "EPOCHS = 30"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:02:53.707513Z",
     "start_time": "2024-03-31T04:02:52.406668Z"
    }
   },
   "id": "61e8a2f9cf20bc05"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomRoBert:\n\tsize mismatch for transformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50006, 768]) from checkpoint, the shape in current model is torch.Size([50005, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[0;32m   2148\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2149\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2150\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[0;32m   2152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2154\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[0;32m   2155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for CustomRoBert:\n\tsize mismatch for transformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50006, 768]) from checkpoint, the shape in current model is torch.Size([50005, 768])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T19:52:44.453677Z",
     "start_time": "2024-03-30T19:52:44.103521Z"
    }
   },
   "id": "6f386808f3769049",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:19<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train LossL 0.018212180059648864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:18<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train LossL 0.013586576584837206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train LossL 0.009156901599412516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:15<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train LossL 0.006248278562818641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:15<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train LossL 0.004728778695033631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train LossL 0.003954299368998472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:32<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train LossL 0.0035107603499321305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [07:47<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train LossL 0.003230769312638382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [07:53<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train LossL 0.003039705314284557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [11:08<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train LossL 0.0029050588830817213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [11:52<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train LossL 0.0028343093133624824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [09:18<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train LossL 0.00268412483978704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [11:12<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train LossL 0.002619573451374504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [07:48<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train LossL 0.0025488297243476048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:06<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train LossL 0.0024851629628854896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:14<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train LossL 0.0024565271420929225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:09<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train LossL 0.0023859358409866922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:58<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train LossL 0.002310341060844095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:36<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train LossL 0.002262280443831183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:38<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train LossL 0.002232380019364593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:42<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Train LossL 0.0021980923473253885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Train LossL 0.0021420298090333596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:57<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Train LossL 0.002149243449141544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:20<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Train LossL 0.0020698368825219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:43<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Train LossL 0.0020635582602004897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:43<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Train LossL 0.0020180448989589586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [06:08<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Train LossL 0.0019579526995024245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:51<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Train LossL 0.0019459201032412947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:36<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Train LossL 0.0019681383459456254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [05:21<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Train LossL 0.0019175230220308613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    \n",
    "    for X, y in tqdm(train_dataloader):\n",
    "        y = y.to(device, dtype=torch.long)\n",
    "        X = {key:val.to(device) for key,val in X.items()}\n",
    "        \n",
    "        output = model(X)\n",
    "\n",
    "        output.squeeze(1)\n",
    "        loss = criterion(output.squeeze(1), y.float())\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {epoch + 1} | Train LossL {running_loss / len(training_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T00:15:00.445554Z",
     "start_time": "2024-03-30T20:54:20.089412Z"
    }
   },
   "id": "3e728d314069f5dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_mini.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T00:15:01.245097Z",
     "start_time": "2024-03-31T00:15:00.446556Z"
    }
   },
   "id": "b2ed53d12d647e2a",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:02:46.349482Z",
     "start_time": "2024-03-31T04:02:45.898577Z"
    }
   },
   "id": "1cc821d107ef0853",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_mini.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:03:02.839739Z",
     "start_time": "2024-03-31T04:03:02.286017Z"
    }
   },
   "id": "d64ecbedfa9a596",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    for X, y in tqdm(val_loader):\n",
    "        y = y.to(device, dtype=torch.long)\n",
    "        X = {key:val.to(device) for key,val in X.items()}\n",
    "\n",
    "        output = model(X)\n",
    "\n",
    "        output.squeeze(1)\n",
    "        loss = criterion(output.squeeze(1), y.float())\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        pred = (torch.sigmoid(output.squeeze(1)) > 0.5)\n",
    "        pred_labels.extend(pred.cpu().numpy().tolist())\n",
    "        true_labels.extend(y.cpu().numpy().tolist())\n",
    "        \n",
    "    val_loss /= len(val_loader)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "    print(f\"Val Loss: {val_loss} | Val Acc: {acc} | Val Precision: {precision} | Val Recall: {recall} | Val F1: {f1}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:09:44.944233Z",
     "start_time": "2024-03-31T04:09:44.940727Z"
    }
   },
   "id": "563158f2f0d2b780",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:39<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.06525788049710761 | Val Acc: 0.9787485242030697 | Val Precision: 0.9881305637982196 | Val Recall: 0.9786921381337252 | Val F1: 0.9833887043189369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate(model, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:10:24.775002Z",
     "start_time": "2024-03-31T04:09:45.452410Z"
    }
   },
   "id": "d599864fb84e783c",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    pred_labels = []\n",
    "\n",
    "    for X in tqdm(test_loader):\n",
    "        X = {key:val.to(device) for key,val in X.items()}\n",
    "\n",
    "        output = model(X)\n",
    "\n",
    "        output.squeeze(1)\n",
    "        pred = (torch.sigmoid(output.squeeze(1)) > 0.5)\n",
    "        pred_labels.extend(pred.cpu().numpy().tolist())\n",
    "        \n",
    "    return pred_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:10:35.091917Z",
     "start_time": "2024-03-31T04:10:35.089382Z"
    }
   },
   "id": "e3083e16495c28d3",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDatasetTest(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataframe.reset_index()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        title = self.dataframe.loc[index]['title']\n",
    "\n",
    "        title = title.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\").replace(\"Ţ\", \"Ț\").replace(\"Ş\", \"Ș\")\n",
    "\n",
    "        title_tensor = self.tokenizer(title, max_length=512, padding='max_length', return_tensors='pt', truncation=True)\n",
    "        return title_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:10:36.524892Z",
     "start_time": "2024-03-31T04:10:36.522378Z"
    }
   },
   "id": "6e02e00c0cff06c5",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_dataset = CustomDatasetTest(df_test, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:10:37.018609Z",
     "start_time": "2024-03-31T04:10:37.009112Z"
    }
   },
   "id": "f4f4264a5db8c654",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:10:37.470596Z",
     "start_time": "2024-03-31T04:10:37.468758Z"
    }
   },
   "id": "6365866f526d007b",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1146/1146 [05:09<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:15:47.649969Z",
     "start_time": "2024-03-31T04:10:37.886109Z"
    }
   },
   "id": "18a1d8ab4f9d31f6",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'id': df_test['id'], 'class': preds})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:15:47.656276Z",
     "start_time": "2024-03-31T04:15:47.650473Z"
    }
   },
   "id": "8445c179d9934e5f",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred_df.to_csv('submission_robert_30E_full.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T04:15:47.691934Z",
     "start_time": "2024-03-31T04:15:47.657280Z"
    }
   },
   "id": "7e4dc27f29f6bc2f",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eba4a9320dc5dae9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
